{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP40S+IyCfu8sIoPzTFVrzR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tmdoi/small-Japanese-LLM-compare/blob/main/benchMarkOrginal_v03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gqVLjbLFxVXd"
      },
      "outputs": [],
      "source": [
        "!pip -q install \"transformers>=4.43\" accelerate torch --upgrade\n",
        "!pip -q install pandas sacrebleu rouge-score fugashi ipadic"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === (Colab セル1) セットアップ & モデル読込 ===\n",
        "try:\n",
        "    import google.colab  # noqa: F401\n",
        "    IN_COLAB = True\n",
        "except Exception:\n",
        "    IN_COLAB = False\n",
        "\n",
        "import sys, subprocess, math, time, re\n",
        "def pip_install(pkgs):\n",
        "    cmd = [sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"--upgrade\"] + pkgs\n",
        "    print(\"Installing:\", \" \".join(pkgs))\n",
        "    subprocess.check_call(cmd)\n",
        "\n",
        "# 必要に応じて有効化してください（初回実行時など）\n",
        "# pip_install([\"transformers>=4.43\", \"accelerate\", \"torch\", \"pandas\", \"sacrebleu\", \"rouge-score\", \"fugashi\", \"ipadic\"])\n",
        "\n",
        "# ---- 以降 Python 本体 ----\n",
        "import torch, pandas as pd\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Dict, Any, Tuple\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from rouge_score import rouge_scorer\n",
        "import sacrebleu\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# 乱数固定（再現性の一助）\n",
        "random.seed(0)\n",
        "np.random.seed(0)\n",
        "if torch.cuda.is_available():\n",
        "    torch.manual_seed(0)\n",
        "    torch.cuda.manual_seed_all(0)\n",
        "\n",
        "# 比較対象モデル（必要に応じて変更可）\n",
        "MODELS = {\n",
        "    \"RakutenAI-2.0-mini-instruct\": \"Rakuten/RakutenAI-2.0-mini-instruct\",\n",
        "    \"TinySwallow-1.5B-Instruct\":   \"SakanaAI/TinySwallow-1.5B-Instruct\",\n",
        "}\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device)\n",
        "\n",
        "@dataclass\n",
        "class GenConfig:\n",
        "    max_new_tokens: int = 256\n",
        "    temperature: float = 0.0   # 再現性重視\n",
        "    top_p: float = 1.0\n",
        "    do_sample: bool = False\n",
        "    num_beams: int = 1\n",
        "\n",
        "GENCFG = GenConfig()\n",
        "\n",
        "def load_model(repo_id: str) -> Tuple[AutoTokenizer, AutoModelForCausalLM]:\n",
        "    tok = AutoTokenizer.from_pretrained(repo_id, use_fast=True)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        repo_id,\n",
        "        torch_dtype=\"auto\",\n",
        "        device_map=\"auto\",   # ColabのGPUに自動割当\n",
        "    )\n",
        "    return tok, model\n",
        "\n",
        "def chat_generate(tokenizer, model, messages: List[Dict[str, str]], cfg: GenConfig = GENCFG):\n",
        "    # 各モデルのchatテンプレートを利用\n",
        "    input_ids = tokenizer.apply_chat_template(\n",
        "        messages, add_generation_prompt=True, return_tensors=\"pt\"\n",
        "    ).to(model.device)\n",
        "\n",
        "    attn = None\n",
        "    if tokenizer.pad_token_id is not None:\n",
        "        attn = input_ids.ne(tokenizer.pad_token_id).long()\n",
        "\n",
        "    t0 = time.perf_counter()\n",
        "    with torch.no_grad():\n",
        "        out_ids = model.generate(\n",
        "            input_ids,\n",
        "            max_new_tokens=cfg.max_new_tokens,\n",
        "            do_sample=cfg.do_sample,\n",
        "            temperature=cfg.temperature,\n",
        "            top_p=cfg.top_p,\n",
        "            num_beams=cfg.num_beams,\n",
        "            attention_mask=attn,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "    dt = time.perf_counter() - t0\n",
        "    gen_ids = out_ids[:, input_ids.shape[1]:]\n",
        "    text = tokenizer.batch_decode(gen_ids, skip_special_tokens=True)[0].strip()\n",
        "    toks = gen_ids.shape[1]\n",
        "    tps = toks / dt if dt > 0 else float(\"nan\")\n",
        "    return text, {\"latency_sec\": dt, \"gen_tokens\": toks, \"tok_per_sec\": tps}\n",
        "\n",
        "# タスク定義（自動採点可能なもの中心）\n",
        "TASKS = [\n",
        "    {\n",
        "        \"name\": \"JA-QA: 富士山の標高\",\n",
        "        \"messages\": [\n",
        "            {\"role\":\"system\",\"content\":\"あなたは有能な日本語アシスタントです。\"},\n",
        "            {\"role\":\"user\",\"content\":\"富士山の標高は？数値と単位で簡潔に答えてください。\"}\n",
        "        ],\n",
        "        # 3776 を数値として含めれば正解扱い（ゆるい判定）\n",
        "        \"judge\": lambda x: (\"3776\" in re.sub(r\"[^\\d]\", \"\", x)) or (\"3,776\" in x) or (\"3776 m\" in x) or (\"3776メートル\" in x),\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"算数: 12×(7+5)\",\n",
        "        \"messages\": [\n",
        "            {\"role\":\"system\",\"content\":\"あなたは計算に正確です。\"},\n",
        "            {\"role\":\"user\",\"content\":\"12×(7+5) の結果だけを半角数字で答えてください。\"}\n",
        "        ],\n",
        "        \"judge\": lambda x: \"144\" in re.sub(r\"[^\\d\\-]\", \"\", x),\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"要約: 5文→1文\",\n",
        "        \"messages\": [\n",
        "            {\"role\":\"system\",\"content\":\"与えられた段落を1文で要約してください。\"},\n",
        "            {\"role\":\"user\",\"content\":\n",
        "             \"奈良公園には多くのシカが生息し、観光客に人気です。\"\n",
        "             \"近年は観光客の増加に伴い、エサの与え方やごみ問題が課題となっています。\"\n",
        "             \"地元自治体はルール啓発と清掃活動を強化しています。\"\n",
        "             \"一方で来園者のマナー向上には時間がかかるとの指摘もあります。\"\n",
        "             \"持続可能な観光の実現に向け、地域と来訪者の協力が求められています。\"\n",
        "            }\n",
        "        ],\n",
        "        \"ref\": \"奈良公園のシカと観光をめぐる課題に対し、自治体と来訪者の協力による持続可能な観光の実現が求められている。\",\n",
        "        \"rougeL\": True\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"翻訳: EN→JA\",\n",
        "        \"messages\": [\n",
        "            {\"role\":\"system\",\"content\":\"次の英文を自然な日本語に翻訳してください。\"},\n",
        "            {\"role\":\"user\",\"content\":\"Edge-friendly small LLMs enable private, low-latency applications without relying on cloud services.\"}\n",
        "        ],\n",
        "        \"ref\": \"エッジ向けの小型LLMは、クラウドサービスに依存せずにプライバシーに配慮した低遅延アプリケーションを可能にする。\",\n",
        "        \"bleu\": True\n",
        "    },\n",
        "]\n",
        "\n",
        "# ROUGE-L スコアラー（※ベンチマーク側でも参照）\n",
        "scorer = rouge_scorer.RougeScorer([\"rougeL\"], use_stemmer=False)\n",
        "\n",
        "# モデル／トークナイザを事前ロードして保持（ベンチマーク側で再利用）\n",
        "print(\"\\n== Loading models ==\")\n",
        "LOADED = {}  # { model_name: (tokenizer, model) }\n",
        "for name, repo in MODELS.items():\n",
        "    print(f\"Loading: {name} ({repo})\")\n",
        "    tok, mdl = load_model(repo)\n",
        "    LOADED[name] = (tok, mdl)\n",
        "\n",
        "print(\"Loaded:\", list(LOADED.keys()))\n"
      ],
      "metadata": {
        "id": "j_3-kpBaxr3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === (Colab セルTaskAdd) 既存TASKSにタスクを追加 ===\n",
        "\n",
        "EXTRA_TASKS = [\n",
        "    {\n",
        "        \"name\": \"JA-QA: 昭和新山の標高\",\n",
        "        \"messages\": [\n",
        "            {\"role\":\"system\",\"content\":\"あなたは有能な日本語アシスタントです。\"},\n",
        "            {\"role\":\"user\",\"content\":\"昭和新山の標高は？数値と単位で簡潔に答えてください。\"}\n",
        "        ],\n",
        "        \"judge\": lambda x: (\"398\" in re.sub(r\"[^\\d]\", \"\", x)) or (\"3776メートル\" in x)\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"JA-QA: 四万十川の長さ\",\n",
        "        \"messages\": [\n",
        "            {\"role\":\"system\",\"content\":\"あなたは有能な日本語アシスタントです。\"},\n",
        "            {\"role\":\"user\",\"content\":\"四万十川の長さは？数値と単位で簡潔に答えてください。\"}\n",
        "        ],\n",
        "        \"judge\": lambda x: (\"196\" in re.sub(r\"[^\\d]\", \"\", x)) or (\"196キロメートル\" in x)\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"算数: 23×19\",\n",
        "        \"messages\": [\n",
        "            {\"role\":\"system\",\"content\":\"あなたは計算に正確です。\"},\n",
        "            {\"role\":\"user\",\"content\":\"23×19 の結果だけを半角数字で答えてください。\"}\n",
        "        ],\n",
        "        \"judge\": lambda x: \"437\" in re.sub(r\"[^\\d\\-]\", \"\", x),\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"要約: 教育\",\n",
        "        \"messages\": [\n",
        "            {\"role\":\"system\",\"content\":\"与えられた段落を1文で要約してください。\"},\n",
        "            {\"role\":\"user\",\"content\":\n",
        "             \"教育は社会の基盤を支える重要な要素であり、\"\n",
        "             \"知識や技術の習得だけでなく、人間性の成長にも寄与する。\"\n",
        "             \"現代社会ではICT活用が進み、新しい学びの形が模索されている。\"\n",
        "            }\n",
        "        ],\n",
        "        \"ref\": \"教育は知識・技術と人間性の成長を支える基盤であり、ICT活用による新しい学びが求められている。\",\n",
        "        \"rougeL\": True\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"翻訳: JA→EN\",\n",
        "        \"messages\": [\n",
        "            {\"role\":\"system\",\"content\":\"次の日本語を自然な英語に翻訳してください。\"},\n",
        "            {\"role\":\"user\",\"content\":\"大阪は日本で三番目に大きな都市です。\"}\n",
        "        ],\n",
        "        \"ref\": \"Osaka is the third largest city in Japan.\",\n",
        "        \"bleu\": True\n",
        "    },\n",
        "]\n",
        "\n",
        "# 既存 TASKS に追加\n",
        "TASKS.extend(EXTRA_TASKS)\n",
        "\n",
        "print(f\"Redefined TASKS: {len(TASKS)} tasks\")\n"
      ],
      "metadata": {
        "id": "FV7s06G4mB67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === (Colab セル2) 日本語対応のベンチマーク実行（セル1は無改変） ===\n",
        "import math, numpy as np, pandas as pd, unicodedata, re\n",
        "import sacrebleu\n",
        "\n",
        "# セル1で定義済みの以下を利用します:\n",
        "# - LOADED: { model_name: (tokenizer, model) }\n",
        "# - TASKS: タスクリスト\n",
        "# - chat_generate(): 生成関数\n",
        "# - GENCFG: 生成設定\n",
        "# ※ セル1の `scorer` は使わず、セル2内で日本語対応スコアラーを用意します。\n",
        "\n",
        "# ----------------------------\n",
        "# 日本語テキストの前処理 & トークナイザ設定（セル2内完結）\n",
        "# ----------------------------\n",
        "def normalize_ja(s: str) -> str:\n",
        "    # 全角半角の揺れ・不要空白などを吸収（必要に応じて調整）\n",
        "    s = unicodedata.normalize(\"NFKC\", s)\n",
        "    s = s.strip()\n",
        "    return s\n",
        "\n",
        "# fugashi (MeCab) が使えれば単語ベース、無ければ文字ベース\n",
        "try:\n",
        "    from fugashi import Tagger\n",
        "    _tagger = Tagger()\n",
        "    def ja_tokens(text: str):\n",
        "        text = normalize_ja(text)\n",
        "        return [m.surface for m in _tagger(text)]\n",
        "    _bleu_tokenize = \"ja-mecab\"  # sacrebleu の日本語用トークナイザ指定\n",
        "    print(\"Tokenizer for ROUGE: fugashi(MeCab) / BLEU: ja-mecab\")\n",
        "except Exception:\n",
        "    def ja_tokens(text: str):\n",
        "        text = normalize_ja(text)\n",
        "        return list(text)  # 文字ベース\n",
        "    _bleu_tokenize = \"char\"       # 文字ベースBLEU\n",
        "    print(\"Tokenizer for ROUGE: char-level / BLEU: char\")\n",
        "\n",
        "# ----------------------------\n",
        "# ROUGE-L（日本語向け）の実装（セル2内完結）\n",
        "# ----------------------------\n",
        "# rouge_score の内部Tokenizerはセル1で固定されている可能性があるため、\n",
        "# ここではLCSに基づくROUGE-L F1をセル2側で実装します。\n",
        "def _lcs_len(a, b):\n",
        "    # a, b: トークン列\n",
        "    # 動的計画法でLCS長を計算（O(n*m)）\n",
        "    n, m = len(a), len(b)\n",
        "    dp = [0]*(m+1)\n",
        "    for i in range(1, n+1):\n",
        "        prev = 0\n",
        "        for j in range(1, m+1):\n",
        "            tmp = dp[j]\n",
        "            if a[i-1] == b[j-1]:\n",
        "                dp[j] = prev + 1\n",
        "            else:\n",
        "                dp[j] = max(dp[j], dp[j-1])\n",
        "            prev = tmp\n",
        "    return dp[m]\n",
        "\n",
        "def rougeL_f1(ref: str, hyp: str) -> float:\n",
        "    ref_toks = ja_tokens(ref)\n",
        "    hyp_toks = ja_tokens(hyp)\n",
        "    if len(ref_toks) == 0 or len(hyp_toks) == 0:\n",
        "        return 0.0\n",
        "    lcs = _lcs_len(ref_toks, hyp_toks)\n",
        "    prec = lcs / len(hyp_toks)\n",
        "    rec  = lcs / len(ref_toks)\n",
        "    if prec + rec == 0:\n",
        "        return 0.0\n",
        "    f1 = (2 * prec * rec) / (prec + rec)\n",
        "    return float(f1)\n",
        "\n",
        "# ----------------------------\n",
        "# 評価関数（セル1のモデル群を利用）\n",
        "# ----------------------------\n",
        "def evaluate_one_loaded(model_name: str, tok_mdl):\n",
        "    tok, mdl = tok_mdl\n",
        "    rows = []\n",
        "    for task in TASKS:\n",
        "        out, stats = chat_generate(tok, mdl, task[\"messages\"])\n",
        "        # 軽いノイズ除去（任意）：丁寧な前置きのカット例\n",
        "        out_clean = re.sub(r\"^(はい、|承知しました。|以下のとおりです。)+\", \"\", out).strip()\n",
        "\n",
        "        row = {\n",
        "            \"model\": model_name,\n",
        "            \"task\": task[\"name\"],\n",
        "            \"output\": out,      # 生出力も保持\n",
        "            **stats\n",
        "        }\n",
        "        # pass@1（セル1定義のjudgeに従う）\n",
        "        if \"judge\" in task:\n",
        "            row[\"pass@1\"] = bool(task[\"judge\"](out_clean))\n",
        "\n",
        "        # ROUGE-L（セル2内で日本語対応計算）\n",
        "        if task.get(\"rougeL\"):\n",
        "            r = rougeL_f1(task[\"ref\"], out_clean)\n",
        "            row[\"ROUGE-L\"] = r\n",
        "\n",
        "        # BLEU（日本語向けトークナイズ設定）\n",
        "        if task.get(\"bleu\"):\n",
        "            bleu = sacrebleu.corpus_bleu(\n",
        "                [normalize_ja(out_clean)],\n",
        "                [[normalize_ja(task[\"ref\"])]],\n",
        "                tokenize=_bleu_tokenize\n",
        "            ).score\n",
        "            row[\"BLEU\"] = bleu\n",
        "\n",
        "        rows.append(row)\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "# ----------------------------\n",
        "# 実行\n",
        "# ----------------------------\n",
        "all_dfs = []\n",
        "for name, tok_mdl in LOADED.items():\n",
        "    print(f\"\\n== Evaluating {name} ==\")\n",
        "    df = evaluate_one_loaded(name, tok_mdl)\n",
        "    display(df[[\"model\",\"task\",\"pass@1\",\"ROUGE-L\",\"BLEU\",\"latency_sec\",\"tok_per_sec\",\"output\"]])\n",
        "    all_dfs.append(df)\n",
        "\n",
        "summary = pd.concat(all_dfs, ignore_index=True)\n",
        "\n",
        "# 集計（タスク別の平均）\n",
        "def safe_mean(xs):\n",
        "    xs = [x for x in xs if x is not None and not (isinstance(x, float) and math.isnan(x))]\n",
        "    return float(np.mean(xs)) if xs else float(\"nan\")\n",
        "\n",
        "report = []\n",
        "for m in summary[\"model\"].unique():\n",
        "    sub = summary[summary[\"model\"]==m]\n",
        "    pass_mean = safe_mean([1.0 if x is True else (0.0 if x is False else None) for x in sub.get(\"pass@1\", []).tolist()])\n",
        "    rouge_mean = safe_mean(sub.get(\"ROUGE-L\", []).tolist())\n",
        "    bleu_mean  = safe_mean(sub.get(\"BLEU\", []).tolist())\n",
        "    tps_mean   = safe_mean(sub.get(\"tok_per_sec\", []).tolist())\n",
        "    lat_mean   = safe_mean(sub.get(\"latency_sec\", []).tolist())\n",
        "    report.append({\n",
        "        \"model\": m,\n",
        "        \"pass@1(mean)\": pass_mean,\n",
        "        \"ROUGE-L(mean)\": rouge_mean,\n",
        "        \"BLEU(mean)\": bleu_mean,\n",
        "        \"tok_per_sec(mean)\": tps_mean,\n",
        "        \"latency_sec(mean)\": lat_mean\n",
        "    })\n",
        "\n",
        "print(\"\\n== Summary ==\")\n",
        "display(pd.DataFrame(report))\n",
        "\n",
        "# 生成条件を変更したい場合（任意）\n",
        "# GENCFG.max_new_tokens = 128\n",
        "# GENCFG.temperature = 0.7\n",
        "# GENCFG.do_sample = True\n",
        "# print(\"New GenConfig:\", GENCFG)\n"
      ],
      "metadata": {
        "id": "QJKX37KblWyZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}