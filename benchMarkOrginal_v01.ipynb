{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPqlor6llaVPGIeMj32YS23",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tmdoi/small-Japanese-LLM-compare/blob/main/benchMarkOrginal_v01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gqVLjbLFxVXd"
      },
      "outputs": [],
      "source": [
        "!pip -q install \"transformers>=4.43\" accelerate torch --upgrade\n",
        "!pip -q install pandas sacrebleu rouge-score fugashi ipadic"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === (Colab 1セル完結) RakutenAI-2.0-mini-instruct vs TinySwallow-1.5B-Instruct 簡易ベンチマーク ===\n",
        "# セットアップ\n",
        "try:\n",
        "    import google.colab  # noqa: F401\n",
        "    IN_COLAB = True\n",
        "except Exception:\n",
        "    IN_COLAB = False\n",
        "\n",
        "import sys, subprocess, math, time, re\n",
        "def pip_install(pkgs):\n",
        "    cmd = [sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"--upgrade\"] + pkgs\n",
        "    print(\"Installing:\", \" \".join(pkgs))\n",
        "    subprocess.check_call(cmd)\n",
        "\n",
        "# 主要ライブラリ\n",
        "#pip_install([\"transformers>=4.43\", \"accelerate\", \"torch\", \"pandas\", \"sacrebleu\", \"rouge-score\", \"fugashi\", \"ipadic\"])\n",
        "\n",
        "# ---- 以降 Python 本体 ----\n",
        "import torch, pandas as pd\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Dict, Any\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from rouge_score import rouge_scorer\n",
        "import sacrebleu\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# 乱数固定（再現性の一助）\n",
        "random.seed(0)\n",
        "np.random.seed(0)\n",
        "if torch.cuda.is_available():\n",
        "    torch.manual_seed(0)\n",
        "    torch.cuda.manual_seed_all(0)\n",
        "\n",
        "# 比較対象モデル（必要に応じて変更可）\n",
        "MODELS = {\n",
        "    \"RakutenAI-2.0-mini-instruct\": \"Rakuten/RakutenAI-2.0-mini-instruct\",\n",
        "    \"TinySwallow-1.5B-Instruct\":   \"SakanaAI/TinySwallow-1.5B-Instruct\",\n",
        "}\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device)\n",
        "\n",
        "@dataclass\n",
        "class GenConfig:\n",
        "    max_new_tokens: int = 256\n",
        "    temperature: float = 0.0   # 再現性重視\n",
        "    top_p: float = 1.0\n",
        "    do_sample: bool = False\n",
        "    num_beams: int = 1\n",
        "\n",
        "GENCFG = GenConfig()\n",
        "\n",
        "def load_model(repo_id: str):\n",
        "    tok = AutoTokenizer.from_pretrained(repo_id, use_fast=True)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        repo_id,\n",
        "        torch_dtype=\"auto\",\n",
        "        device_map=\"auto\",   # ColabのGPUに自動割当\n",
        "    )\n",
        "    return tok, model\n",
        "\n",
        "def chat_generate(tokenizer, model, messages: List[Dict[str, str]], cfg: GenConfig = GENCFG):\n",
        "    # 各モデルのchatテンプレートを利用\n",
        "    input_ids = tokenizer.apply_chat_template(\n",
        "        messages, add_generation_prompt=True, return_tensors=\"pt\"\n",
        "    ).to(model.device)\n",
        "\n",
        "    attn = None\n",
        "    if tokenizer.pad_token_id is not None:\n",
        "        attn = input_ids.ne(tokenizer.pad_token_id).long()\n",
        "\n",
        "    t0 = time.perf_counter()\n",
        "    with torch.no_grad():\n",
        "        out_ids = model.generate(\n",
        "            input_ids,\n",
        "            max_new_tokens=cfg.max_new_tokens,\n",
        "            do_sample=cfg.do_sample,\n",
        "            temperature=cfg.temperature,\n",
        "            top_p=cfg.top_p,\n",
        "            num_beams=cfg.num_beams,\n",
        "            attention_mask=attn,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "    dt = time.perf_counter() - t0\n",
        "    gen_ids = out_ids[:, input_ids.shape[1]:]\n",
        "    text = tokenizer.batch_decode(gen_ids, skip_special_tokens=True)[0].strip()\n",
        "    toks = gen_ids.shape[1]\n",
        "    tps = toks / dt if dt > 0 else float(\"nan\")\n",
        "    return text, {\"latency_sec\": dt, \"gen_tokens\": toks, \"tok_per_sec\": tps}\n",
        "\n",
        "# 簡易タスク（自動採点可能なもの中心）\n",
        "TASKS = [\n",
        "    {\n",
        "        \"name\": \"JA-QA: 富士山の標高\",\n",
        "        \"messages\": [\n",
        "            {\"role\":\"system\",\"content\":\"あなたは有能な日本語アシスタントです。\"},\n",
        "            {\"role\":\"user\",\"content\":\"富士山の標高は？数値と単位で簡潔に答えてください。\"}\n",
        "        ],\n",
        "        # 3776 を数値として含めれば正解扱い（ゆるい判定）\n",
        "        \"judge\": lambda x: (\"3776\" in re.sub(r\"[^\\d]\", \"\", x)) or (\"3,776\" in x) or (\"3776 m\" in x) or (\"3776メートル\" in x),\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"算数: 12×(7+5)\",\n",
        "        \"messages\": [\n",
        "            {\"role\":\"system\",\"content\":\"あなたは計算に正確です。\"},\n",
        "            {\"role\":\"user\",\"content\":\"12×(7+5) の結果だけを半角数字で答えてください。\"}\n",
        "        ],\n",
        "        \"judge\": lambda x: \"144\" in re.sub(r\"[^\\d\\-]\", \"\", x),\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"要約: 5文→1文\",\n",
        "        \"messages\": [\n",
        "            {\"role\":\"system\",\"content\":\"与えられた段落を1文で要約してください。\"},\n",
        "            {\"role\":\"user\",\"content\":\n",
        "             \"奈良公園には多くのシカが生息し、観光客に人気です。\"\n",
        "             \"近年は観光客の増加に伴い、エサの与え方やごみ問題が課題となっています。\"\n",
        "             \"地元自治体はルール啓発と清掃活動を強化しています。\"\n",
        "             \"一方で来園者のマナー向上には時間がかかるとの指摘もあります。\"\n",
        "             \"持続可能な観光の実現に向け、地域と来訪者の協力が求められています。\"\n",
        "            }\n",
        "        ],\n",
        "        \"ref\": \"奈良公園のシカと観光をめぐる課題に対し、自治体と来訪者の協力による持続可能な観光の実現が求められている。\",\n",
        "        \"rougeL\": True\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"翻訳: EN→JA\",\n",
        "        \"messages\": [\n",
        "            {\"role\":\"system\",\"content\":\"次の英文を自然な日本語に翻訳してください。\"},\n",
        "            {\"role\":\"user\",\"content\":\"Edge-friendly small LLMs enable private, low-latency applications without relying on cloud services.\"}\n",
        "        ],\n",
        "        \"ref\": \"エッジ向けの小型LLMは、クラウドサービスに依存せずにプライバシーに配慮した低遅延アプリケーションを可能にする。\",\n",
        "        \"bleu\": True\n",
        "    },\n",
        "]\n",
        "\n",
        "scorer = rouge_scorer.RougeScorer([\"rougeL\"], use_stemmer=False)\n",
        "\n",
        "def evaluate_one(model_name, repo):\n",
        "    tok, mdl = load_model(repo)\n",
        "    rows = []\n",
        "    for task in TASKS:\n",
        "        out, stats = chat_generate(tok, mdl, task[\"messages\"])\n",
        "        row = {\n",
        "            \"model\": model_name,\n",
        "            \"task\": task[\"name\"],\n",
        "            \"output\": out,\n",
        "            **stats\n",
        "        }\n",
        "        if \"judge\" in task:\n",
        "            row[\"pass@1\"] = bool(task[\"judge\"](out))\n",
        "        if task.get(\"rougeL\"):\n",
        "            r = scorer.score(task[\"ref\"], out)[\"rougeL\"].fmeasure\n",
        "            row[\"ROUGE-L\"] = r\n",
        "        if task.get(\"bleu\"):\n",
        "            bleu = sacrebleu.corpus_bleu([out], [[task[\"ref\"]]]).score\n",
        "            row[\"BLEU\"] = bleu\n",
        "        rows.append(row)\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "# 実行\n",
        "all_dfs = []\n",
        "for name, repo in MODELS.items():\n",
        "    print(f\"\\n== Evaluating {name} ==\")\n",
        "    df = evaluate_one(name, repo)\n",
        "    display(df[[\"model\",\"task\",\"pass@1\",\"ROUGE-L\",\"BLEU\",\"latency_sec\",\"tok_per_sec\",\"output\"]])\n",
        "    all_dfs.append(df)\n",
        "\n",
        "summary = pd.concat(all_dfs, ignore_index=True)\n",
        "\n",
        "# 集計（タスク別の平均）\n",
        "def safe_mean(xs):\n",
        "    xs = [x for x in xs if x is not None and not (isinstance(x, float) and math.isnan(x))]\n",
        "    return float(np.mean(xs)) if xs else float(\"nan\")\n",
        "\n",
        "report = []\n",
        "for m in summary[\"model\"].unique():\n",
        "    sub = summary[summary[\"model\"]==m]\n",
        "    pass_mean = safe_mean([1.0 if x is True else (0.0 if x is False else None) for x in sub.get(\"pass@1\", []).tolist()])\n",
        "    rouge_mean = safe_mean(sub.get(\"ROUGE-L\", []).tolist())\n",
        "    bleu_mean  = safe_mean(sub.get(\"BLEU\", []).tolist())\n",
        "    tps_mean   = safe_mean(sub.get(\"tok_per_sec\", []).tolist())\n",
        "    lat_mean   = safe_mean(sub.get(\"latency_sec\", []).tolist())\n",
        "    report.append({\n",
        "        \"model\": m,\n",
        "        \"pass@1(mean)\": pass_mean,\n",
        "        \"ROUGE-L(mean)\": rouge_mean,\n",
        "        \"BLEU(mean)\": bleu_mean,\n",
        "        \"tok_per_sec(mean)\": tps_mean,\n",
        "        \"latency_sec(mean)\": lat_mean\n",
        "    })\n",
        "\n",
        "print(\"\\n== Summary ==\")\n",
        "display(pd.DataFrame(report))\n",
        "\n",
        "# 生成長・温度などを変えたい場合の例（任意で有効化）\n",
        "# GENCFG.max_new_tokens = 128\n",
        "# GENCFG.temperature = 0.7\n",
        "# GENCFG.do_sample = True\n",
        "# print(\"New GenConfig:\", GENCFG)\n"
      ],
      "metadata": {
        "id": "j_3-kpBaxr3h"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}